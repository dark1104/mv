from pyflink.common import Configuration, Types
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.datastream.connectors.kafka import FlinkKafkaConsumer
from pyflink.common.serialization import SimpleStringSchema
from pyflink.table import StreamTableEnvironment, DataTypes, Schema, Row
from pyflink.datastream.functions import MapFunction
import json
from datetime import datetime

class JsonToRowMap(MapFunction):
    def map(self, value):
        data = json.loads(value)
        return Row(
            data.get("date", ""),
            data.get("sourceIPv4Address", ""),
            data.get("destinationIPv4Address", ""),
            data.get("protocolName", ""),
            data.get("sourceTransportPort", 0),
            data.get("destinationTransportPort", 0),
            data.get("octetDeltaCount", 0),
            data.get("packetDeltaCount", 0),
            data.get("flowStartMilliseconds", 0),
            data.get("flowEndMilliseconds", 0),
            data.get("mac_source", ""),
            data.get("mac_destination", ""),
            data.get("application_name", ""),
            data.get("http_url", ""),
            data.get("https_url_certificate", ""),
            data.get("datalink_vlan", 0),
            data.get("flow_start_time", ""),
            data.get("flow_end_time", ""),
            data.get("bytes_accumulated", 0),
            data.get("tcp_retransmits", 0),
            data.get("tcp_rst", 0),
            data.get("tcp_fin", 0),
            data.get("bytes_per_packet", 0.0),
            data.get("flow_duration_ms", 0),
            data.get("flow_direction", ""),
            data.get("profile_name", "")
        )

def main():
    config = Configuration()
    config.set_string(
        "pipeline.jars",
        "file:///opt/flink/lib/flink-connector-kafka-1.17.1.jar,file:///opt/flink/lib/flink-sql-connector-clickhouse-1.17.1-9.jar"
    )

    env = StreamExecutionEnvironment.get_execution_environment()
    t_env = StreamTableEnvironment.create(env, environment_settings=None, configuration=config)

    properties = {
        'bootstrap.servers': '192.168.91.37:9092',
        'group.id': 'flink'
    }

    consumer = FlinkKafkaConsumer(
        'flows',
        SimpleStringSchema(),
        properties
    )

    stream = env.add_source(consumer)

    typed_stream = stream.map(JsonToRowMap(), output_type=Types.ROW_NAMED(
        ["date", "sourceIPv4Address", "destinationIPv4Address", "protocolName",
         "sourceTransportPort", "destinationTransportPort", "octetDeltaCount", "packetDeltaCount",
         "flowStartMilliseconds", "flowEndMilliseconds", "mac_source", "mac_destination",
         "application_name", "http_url", "https_url_certificate", "datalink_vlan",
         "flow_start_time", "flow_end_time", "bytes_accumulated", "tcp_retransmits",
         "tcp_rst", "tcp_fin", "bytes_per_packet", "flow_duration_ms",
         "flow_direction", "profile_name"],
        [Types.STRING(), Types.STRING(), Types.STRING(), Types.STRING(),
         Types.INT(), Types.INT(), Types.INT(), Types.INT(),
         Types.LONG(), Types.LONG(), Types.STRING(), Types.STRING(),
         Types.STRING(), Types.STRING(), Types.STRING(), Types.INT(),
         Types.STRING(), Types.STRING(), Types.INT(), Types.INT(),
         Types.INT(), Types.INT(), Types.FLOAT(), Types.INT(),
         Types.STRING(), Types.STRING()]
    ))

    schema = Schema.new_builder() \
        .column("date", DataTypes.STRING()) \
        .column("sourceIPv4Address", DataTypes.STRING()) \
        .column("destinationIPv4Address", DataTypes.STRING()) \
        .column("protocolName", DataTypes.STRING()) \
        .column("sourceTransportPort", DataTypes.INT()) \
        .column("destinationTransportPort", DataTypes.INT()) \
        .column("octetDeltaCount", DataTypes.INT()) \
        .column("packetDeltaCount", DataTypes.INT()) \
        .column("flowStartMilliseconds", DataTypes.BIGINT()) \
        .column("flowEndMilliseconds", DataTypes.BIGINT()) \
        .column("mac_source", DataTypes.STRING()) \
        .column("mac_destination", DataTypes.STRING()) \
        .column("application_name", DataTypes.STRING()) \
        .column("http_url", DataTypes.STRING()) \
        .column("https_url_certificate", DataTypes.STRING()) \
        .column("datalink_vlan", DataTypes.INT()) \
        .column("flow_start_time", DataTypes.STRING()) \
        .column("flow_end_time", DataTypes.STRING()) \
        .column("bytes_accumulated", DataTypes.INT()) \
        .column("tcp_retransmits", DataTypes.INT()) \
        .column("tcp_rst", DataTypes.INT()) \
        .column("tcp_fin", DataTypes.INT()) \
        .column("bytes_per_packet", DataTypes.FLOAT()) \
        .column("flow_duration_ms", DataTypes.INT()) \
        .column("flow_direction", DataTypes.STRING()) \
        .column("profile_name", DataTypes.STRING()) \
        .build()

    table = t_env.from_data_stream(typed_stream, schema)
    t_env.create_temporary_view("enriched_flows", table)

    t_env.execute_sql("""
        CREATE TABLE clickhouse_sink (
            date STRING,
            sourceIPv4Address STRING,
            destinationIPv4Address STRING,
            protocolName STRING,
            sourceTransportPort INT,
            destinationTransportPort INT,
            octetDeltaCount INT,
            packetDeltaCount INT,
            flowStartMilliseconds BIGINT,
            flowEndMilliseconds BIGINT,
            mac_source STRING,
            mac_destination STRING,
            application_name STRING,
            http_url STRING,
            https_url_certificate STRING,
            datalink_vlan INT,
            flow_start_time STRING,
            flow_end_time STRING,
            bytes_accumulated INT,
            tcp_retransmits INT,
            tcp_rst INT,
            tcp_fin INT,
            bytes_per_packet FLOAT,
            flow_duration_ms INT,
            flow_direction STRING,
            profile_name STRING
        ) WITH (
            'connector' = 'clickhouse',
            'url' = 'jdbc:clickhouse://172.27.0.6:8123/default',
            'table-name' = 'flows_raw',
            'username' = 'default',
            'password' = '',
            'sink.batch-size' = '1000',
            'sink.flush-interval' = '1s',
            'sink.max-retries' = '3'
        )
    """)

    t_env.execute_sql("""
        INSERT INTO clickhouse_sink
        SELECT * FROM enriched_flows
    """)

if __name__ == "__main__":
    main()
